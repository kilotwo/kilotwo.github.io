<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[OpenCV学习之路（三）——轮廓查找与多边形绘制]]></title>
    <url>%2F2019%2F04%2F12%2FOpenCV%2FOpenCV-3%2F</url>
    <content type="text"><![CDATA[查找与绘制轮廓一个轮廓一般对应一系列点，即一条曲线。在OpenCV中，可以用findContours()函数从二值图像查找轮廓。 查找findContours()函数findContours(srcImage, contours, hierarchy, RETR_CCOMP, CHAIN_APPROX_SIMPLE); 第一个参数，输入图像，即原图像，填Mat单通道图像。可使用compare()、inrange()、threshold()、adaptivethreshold()、canny()等函数或彩色图创建二进制图像。 第二个参数，检测到的轮廓、函数调用后的运算结果。每个轮廓存储为一个点向量，即用point类型的vector表示。 第三个参数，可选的输出向量，包含图像的拓扑信息。每个轮廓contours[i]对应4个hierarchy元素hierarchy[i][0]~hierarchy[i][3],分别为后一个轮廓、前一个轮廓、父轮廓、内嵌轮廓的索引编号。 第四个参数，轮廓检索模式。只检测最外层、提取所有轮廓、提取所有轮廓并组织双层结构、提取所有轮廓并重新建立网状轮廓结构。 第五个参数 轮廓的近似方法。 绘制drawContours()函数drawContours(dstImage, contours, index, color, FILLED, 8, hierarchy); 第一个轮廓，目标图像，填Mat对象 第二个参数，所有的输入轮廓，每个轮廓存储为一个点向量，即用point类型的vector表示。 第三个参数，轮廓绘制的指示变量，如果其为负值，则绘制所有轮廓。 第四个参数，轮廓的颜色 第五个参数，粗细度 第六个参数，线条类型 第七个参数，hierarchy 查找并绘制轮廓123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100//---------------------------------【头文件、命名空间包含部分】----------------------------// 描述：包含程序所使用的头文件和命名空间//------------------------------------------------------------------------------------------------#include &lt;opencv2/opencv.hpp&gt;#include &quot;opencv2/highgui/highgui.hpp&quot;#include &quot;opencv2/imgproc/imgproc.hpp&quot;using namespace cv;using namespace std;//-----------------------------------【main( )函数】--------------------------------------------// 描述：控制台应用程序的入口函数，我们的程序从这里开始//-------------------------------------------------------------------------------------------------int main(int argc, char** argv)&#123; // 【1】载入原始图，且必须以二值图模式载入 Mat srcImage = imread(&quot;1.jpg&quot;, 0); imshow(&quot;原始图&quot;, srcImage); //【2】初始化结果图 Mat dstImage = Mat::zeros(srcImage.rows, srcImage.cols, CV_8UC3); //【3】srcImage取大于阈值119的那部分 srcImage = srcImage &gt; 119; imshow(&quot;取阈值后的原始图&quot;, srcImage); //【4】定义轮廓和层次结构 vector&lt;vector&lt;Point&gt; &gt; contours; vector&lt;Vec4i&gt; hierarchy; //【5】查找轮廓 //OpenCV3版为： findContours(srcImage, contours, hierarchy, RETR_CCOMP, CHAIN_APPROX_SIMPLE); // 【6】遍历所有顶层的轮廓， 以随机颜色绘制出每个连接组件颜色 int index = 0; for (; index &gt;= 0; index = hierarchy[index][0]) &#123; Scalar color(rand() &amp; 255, rand() &amp; 255, rand() &amp; 255); //此句代码的OpenCV3版为： drawContours(dstImage, contours, index, color, FILLED, 8, hierarchy); &#125; //【7】显示最后的轮廓图 imshow(&quot;轮廓图&quot;, dstImage); waitKey(0);&#125; 使用多边形把轮廓包围在实际应用中，常常会有将检测到的轮廓用多边形表示出来的需求。比如在一个全家福中，我想用一个矩形框将我自己的头像框出来，这样就需要这方面的知识了。 OpenCv这方面的函数总结如下： 逼近多边形曲线：approxPolyDP():基于RDP算法实现，目的是减少多边形轮廓点数，方面秒点。输入二维点集，输出多边形逼近结果， 返回指定点集轮廓最小矩形边界：boundingRect() 寻找给定的点集可旋转的最小包围矩形：minAreaRect() 寻找最小包围圆形：minEnclosingCircle() 用椭圆拟合二维点集：fitEllipse() 步骤如下： 首先将图像变为二值图像 发现轮廓，找到图像轮廓 通过API在轮廓点上找到最小包含矩形和圆，旋转矩形与椭圆 绘制它们 下面给出这些函数用法的综合案例。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174#include &quot;opencv2/highgui/highgui.hpp&quot;#include &quot;opencv2/imgproc/imgproc.hpp&quot;#include &lt;iostream&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;using namespace cv;using namespace std;Mat src; Mat src_gray;int thresh = 100;int max_thresh = 255;RNG rng(12345);/// 函数声明void thresh_callback(int, void*);/** @主函数 */int main(int argc, char** argv)&#123; /// 载入原图像, 返回3通道图像 src = imread(&quot;123.jpg&quot;, 1); /// 转化成灰度图像并进行平滑 cvtColor(src, src_gray, CV_BGR2GRAY); blur(src_gray, src_gray, Size(3, 3)); /// 创建窗口 char* source_window = &quot;Source&quot;; namedWindow(source_window, CV_WINDOW_AUTOSIZE); imshow(source_window, src); createTrackbar(&quot; Threshold:&quot;, &quot;Source&quot;, &amp;thresh, max_thresh, thresh_callback); thresh_callback(0, 0); waitKey(0); return(0);&#125;/** @thresh_callback 函数 */void thresh_callback(int, void*)&#123; Mat threshold_output; vector&lt;vector&lt;Point&gt; &gt; contours; vector&lt;Vec4i&gt; hierarchy; /// 使用Threshold检测边缘 threshold(src_gray, threshold_output, thresh, 255, THRESH_BINARY); /// 找到轮廓 findContours(threshold_output, contours, hierarchy, CV_RETR_TREE, CV_CHAIN_APPROX_SIMPLE, Point(0, 0)); /// 多边形 vector&lt;vector&lt;Point&gt; &gt; contours_poly(contours.size()); //矩形 vector&lt;Rect&gt; boundRect(contours.size()); //圆心和半径 vector&lt;Point2f&gt;center(contours.size()); vector&lt;float&gt;radius(contours.size()); for (int i = 0; i &lt; contours.size(); i++) &#123; //逼近多边形曲线 approxPolyDP(Mat(contours[i]), contours_poly[i], 3, true); //将逼近后点集传入返回最小矩形边界 boundRect[i] = boundingRect(Mat(contours_poly[i])); //将逼近后点集传入，返回最小包围圆形圆心和半径 minEnclosingCircle(contours_poly[i], center[i], radius[i]); &#125; /// 画多边形轮廓 + 包围的矩形框 + 圆形框 Mat drawing = Mat::zeros(threshold_output.size(), CV_8UC3); for (int i = 0; i&lt; contours.size(); i++) &#123; Scalar color = Scalar(rng.uniform(0, 255), rng.uniform(0, 255), rng.uniform(0, 255)); //画多边形 drawContours(drawing, contours_poly, i, color, 1, 8, vector&lt;Vec4i&gt;(), 0, Point()); //画矩形 rectangle(drawing, boundRect[i].tl(), boundRect[i].br(), color, 2, 8, 0); //画圆 circle(drawing, center[i], (int)radius[i], color, 2, 8, 0); &#125; /// 显示在一个窗口 namedWindow(&quot;Contours&quot;, CV_WINDOW_AUTOSIZE); imshow(&quot;Contours&quot;, drawing);&#125;]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV学习之路（十）————改进]]></title>
    <url>%2F2019%2F04%2F12%2FOpenCV%2FOpenCV%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%E6%94%B9%E8%BF%9B%2F</url>
    <content type="text"><![CDATA[在opencv3中camshift详解（一）中，已经知道是求得反向投影之后再进行camshift追踪，反向投影是以直方图为参照产生的概率分布图，以人脸追踪为例，越可能是人脸的地方，像素的值就越大越“重”，此时对反向投影使用meanshift，搜索框会向图像的“重心”移动，可以预见搜索框最终会停留在人脸可能性最大的地方。输入下一帧图像，重复上述过程，如果人脸位置改变，则反向投影也会改变，搜索框会移动至新的“重心”，搜索框显示在连续图像上就达成了“人脸追踪”效果。 可以看出追踪效果的质量依赖于反向投影的准确性，如果追踪目标和背景的色调（Hue）相近，则会影响到反向投影，造成追踪效果不佳。 因此一种优化 Mean Shift 算法的方法就是利用运动估计器对运动目标的位置进行预测，使得搜索的起点靠近最终匹配点。 1) 光流法虽对环境有很好的适应性，可同时适用于静态背景和动态背景，但 其抗噪能力差，计算复杂度高，耗时较长，导致这种方法的实时性和实用性较差。 2) 帧间差分法的算法简单、运算速度快、对动态背景的适应性很好，但检测 出的运动目标容易产生空洞、边缘缺损或前后分裂拉长，严重情况下同一目标会检 测出两个不同的目标或两帧图像相互重叠而完全被检测不出来。 3) 背景差分法相对于帧间差分法来说，虽可以克服“空洞”的现象，提取出 的运动目标轮廓也较完整，但它对场景中光线的变化特别敏感，背景更新相对较慢， 存在物体的动静转换产生“鬼影”等问题。]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV学习之路(二)——模板匹配]]></title>
    <url>%2F2019%2F04%2F11%2FOpenCV%2FOpenCV%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E6%A8%A1%E7%89%88%E5%8C%B9%E9%85%8D%2F</url>
    <content type="text"><![CDATA[什么是模版匹配？模板匹配，就是在一幅图像中寻找另一幅模板图像最匹配（也就是最相似）的部分的技术。说的有点抽象，下面给个例子说明就很明白了。 其实模板匹配实现的思想也是很简单很暴力的，就是拿着模板图片（头像）在原图中从左上至右下依次滑动，直到遇到某个区域的相似度低于我们设定的阈值，那么我们就认为该区域与模板匹配了，也就是我们找到了头像的位置，并把它标记出来。OpenCV中是通过MtachTemplate函数完成匹配和模版重叠图像区域。matchTemplate(img, templ, result, CV_TM_SQDIFF_NORMED);第一个参数，待搜索的图像第二个参数，搜索模版第三个参数，比较结果的映射图像，必须是单通道、32位浮点数图像第四个参数，指定的匹配方法，OpenCV提供了6种图像匹配方法1。平方差匹配法 2.归一化平方差匹配法 3.相关匹配法 4。归一化相关匹配法 5.系数匹配法 6.化相关系数匹配法越复杂的算法计算量越大速度越慢。注意的是，模板配在原图抠出模板图的形式下准确率才比较高，不然的话可能准确度就不太高了。 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495//---------------------------------【头文件、命名空间包含部分】----------------------------// 描述：包含程序所使用的头文件和命名空间//------------------------------------------------------------------------------------------------#include &quot;opencv2/highgui/highgui.hpp&quot;#include &quot;opencv2/imgproc/imgproc.hpp&quot;using namespace cv;//-----------------------------------【宏定义部分】-------------------------------------------- // 描述：定义一些辅助宏 //------------------------------------------------------------------------------------------------ #define WINDOW_NAME1 &quot;【原始图片】&quot; //为窗口标题定义的宏 #define WINDOW_NAME2 &quot;【匹配窗口】&quot; //为窗口标题定义的宏 //-----------------------------------【全局变量声明部分】------------------------------------// 描述：全局变量的声明//-----------------------------------------------------------------------------------------------Mat g_srcImage; Mat g_templateImage; Mat g_resultImage;int g_nMatchMethod;int g_nMaxTrackbarNum = 5;//-----------------------------------【全局函数声明部分】--------------------------------------// 描述：全局函数的声明//-----------------------------------------------------------------------------------------------void on_Matching(int, void*);//-----------------------------------【main( )函数】--------------------------------------------// 描述：控制台应用程序的入口函数，我们的程序从这里开始执行//-----------------------------------------------------------------------------------------------int main()&#123; //【1】载入原图像和模板块 g_srcImage = imread(&quot;1.jpg&quot;, 1); g_templateImage = imread(&quot;2.jpg&quot;, 1); //【2】创建窗口 namedWindow(WINDOW_NAME1, WINDOW_AUTOSIZE); namedWindow(WINDOW_NAME2, WINDOW_AUTOSIZE); //【3】创建滑动条并进行一次初始化 createTrackbar(&quot;方法&quot;, WINDOW_NAME1, &amp;g_nMatchMethod, g_nMaxTrackbarNum, on_Matching); on_Matching(g_nMatchMethod, 0); waitKey(0); return 0;&#125;//-----------------------------------【on_Matching( )函数】--------------------------------// 描述：回调函数//-------------------------------------------------------------------------------------------void on_Matching(int, void*)&#123; //【1】给局部变量初始化 Mat srcImage; g_srcImage.copyTo(srcImage); //【2】初始化用于结果输出的矩阵 int resultImage_rows = g_srcImage.rows - g_templateImage.rows + 1; int resultImage_cols = g_srcImage.cols - g_templateImage.cols + 1; g_resultImage.create(resultImage_rows, resultImage_cols, CV_32FC1); //【3】进行匹配和归一化 matchTemplate(g_srcImage, g_templateImage, g_resultImage, g_nMatchMethod); normalize(g_resultImage, g_resultImage, 0, 1, NORM_MINMAX, -1, Mat()); //【4】通过函数 minMaxLoc 定位最匹配的位置 double minValue; double maxValue; Point minLocation; Point maxLocation; Point matchLocation; minMaxLoc(g_resultImage, &amp;minValue, &amp;maxValue, &amp;minLocation, &amp;maxLocation); //求出匹配度 最小值 和 最大值 最小值位置和最大值位置 //【5】对于方法 SQDIFF 和 SQDIFF_NORMED, 越小的数值有着更高的匹配结果. 而其余的方法, 数值越大匹配效果越好 //OpenCV3版为： if (g_nMatchMethod == TM_SQDIFF || g_nMatchMethod == TM_SQDIFF_NORMED) &#123; matchLocation = minLocation; &#125; else &#123; matchLocation = maxLocation; &#125; //【6】绘制出矩形，并显示最终结果 输入对角线两个点位置 rectangle(srcImage, matchLocation, Point(matchLocation.x + g_templateImage.cols, matchLocation.y + g_templateImage.rows), Scalar(0, 0, 255), 2, 8, 0); rectangle(g_resultImage, matchLocation, Point(matchLocation.x + g_templateImage.cols, matchLocation.y + g_templateImage.rows), Scalar(0, 0, 255), 2, 8, 0); imshow(WINDOW_NAME1, srcImage); imshow(WINDOW_NAME2, g_resultImage);&#125;]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV学习之路(八)——光流跟踪]]></title>
    <url>%2F2019%2F04%2F11%2FOpenCV%2FOpenCV%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%85%AB%EF%BC%89%E2%80%94%E2%80%94%E5%85%89%E6%B5%81%E8%BF%BD%E8%B8%AA%2F</url>
    <content type="text"><![CDATA[一、基于特征点的目标跟踪的一般方法基于特征点的跟踪算法大致可以分为以下步骤： 1）探测当前帧的特征点； 2）通过当前帧和下一帧灰度比较，估计当前帧特征点在下一帧的位置； 3）过滤位置不变的特征点，余下的点就是目标了。 很显然，基于特征点的目标跟踪算法和1），2）两个步骤有关。特征点可以是Harris角点，也可以是边缘点等等，而估计下一帧位置的方法也有不少，比如这里要讲的光流法，也可以是卡尔曼滤波法。本文中，用改进的Harris角点提取特征点，用Lucas-Kanade光流法实现目标跟踪。 二、光流法 1.首先是假设条件： （1）亮度恒定，就是同一点随着时间的变化，其亮度不会发生改变。这是基本光流法的假定（所有光流法变种都必须满足），用于得到光流法基本方程； （2）小运动，这个也必须满足，就是时间的变化不会引起位置的剧烈变化，这样灰度才能对位置求偏导（换句话说，小运动情况下我们才能用前后帧之间单位位置变化引起的灰度变化去近似灰度对位置的偏导数），这也是光流法不可或缺的假定； （3）空间一致，一个场景上邻近的点投影到图像上也是邻近点，且邻近点速度一致。这是Lucas-Kanade光流法特有的假定，因为光流法基本方程约束只有一个，而要求x，y方向的速度，有两个未知变量。我们假定特征点邻域内做相似运动，就可以连立n多个方程求取x，y方向的速度（n为特征点邻域总点数，包括该特征点）。 2.方程求解 多个方程求两个未知变量，又是线性方程，很容易就想到用最小二乘法，事实上opencv也是这么做的。其中，最小误差平方和为最优化指标。 3.好吧，前面说到了小运动这个假定，聪明的你肯定很不爽了，目标速度很快那这货不是二掉了。幸运的是多尺度能解决这个问题。首先，对每一帧建立一个高斯金字塔，最大尺度图片在最顶层，原始图片在底层。然后，从顶层开始估计下一帧所在位置，作为下一层的初始位置，沿着金字塔向下搜索，重复估计动作，直到到达金字塔的底层。聪明的你肯定发现了：这样搜索不仅可以解决大运动目标跟踪，也可以一定程度上解决孔径问题（相同大小的窗口能覆盖大尺度图片上尽量多的角点，而这些角点无法在原始图片上被覆盖）。 LK算法只需要每个感兴趣点周围小窗口的局部信息，但是较大的运动会将点移除这个小窗口，从而造成算法无法再找到这些点。金字塔的LK算法可以解决这个问题，即从金字塔的最高层（细节最少）开始向金字塔的最低层（丰富的细节）进行跟踪。跟踪图像金字塔允许小窗口部或较大的运动。 在开始跟踪前，首先要在初始帧中检测特征点，之后在下一帧中尝试跟踪这些点。你必须找到新的图像帧中这些点的位置，因此，你必须在特征点的先前位置附近进行搜索，以找到下一帧中它的新位置。这正是cv::calcOpticalFlowPyrLK函数所实现的工作。你输入两个连续的图像帧以及第一幅图像中检测到的特征点数组，该函数将返回一组新的特征点为位置。为了跟踪完整的序列，你需要在帧与帧之间重复这个过程，不可避免地你也会丢失其中一些点，于是被跟踪的特征点数目会减少。为了解决这个问题，我们可以不时地检测新的特征值。 三、OpenCV光流实现123456789101112 calcOpticalFlowPyrLK( InputArray prevImg, InputArray nextImg, InputArray prevPts, InputOutputArray nextPts, OutputArray status, OutputArray err, Size winSize = Size(21,21), int maxLevel = 3, TermCriteria criteria = TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 0.01), int flags = 0, double minEigThreshold = 1e-4 );prevImg：第一帧（跟踪图像的前一帧，一般是定位特征点） nextImg： 第二帧/当前帧 prev_Pts：第一帧特征点集next_Pts：计算输出的第二帧光流特征点集status ：状态标志位，如果对应特征的光流被发现，数组中的每一个元素都被设置为 1， 否则设置为 0。 err：双精度数组，包含原始图像碎片与移动点之间的误差。 四、完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#include&lt;opencv2\opencv.hpp&gt;using namespace cv;using namespace std;void detectFeatures(Mat &amp;gray);void drawFeatures(Mat &amp;frame);void KLTtracker();void drawTrackLine();vector&lt;Point2f&gt;features;//shi-Tomasi角点检测特征数据vector&lt;Point2f&gt;iniPoints;//初始化特征数据vector&lt;Point2f&gt;fpts[2];//保存当前帧和前一帧的特征点位置vector&lt;uchar&gt;status;//特征点跟踪成功标志位vector&lt;float&gt;errors;//跟踪误差Mat frame, gray, pregray;int main(int arc, char** argv) &#123; VideoCapture capture(&quot;006.mp4&quot;); namedWindow(&quot;output&quot;, CV_WINDOW_AUTOSIZE); capture.read(frame);//因为摄像头打开会有延迟，所以提前打开一帧 while (capture.read(frame)) &#123; imshow(&quot;frame&quot;, frame); cvtColor(frame, gray, CV_BGR2GRAY); if (fpts[0].size() &lt; 10) &#123; detectFeatures(gray); fpts[0] = features; iniPoints = features; &#125; if (pregray.empty()) &#123; gray.copyTo(pregray); &#125; KLTtracker(); drawFeatures(frame); //光流跟踪后把当前帧当作前一帧，再与下一帧进行匹配跟踪 gray.copyTo(pregray); imshow(&quot;output&quot;, frame); char c = waitKey(50); if (c == 27) &#123; break; &#125; &#125; capture.release(); waitKey(0); return 0;&#125;void detectFeatures(Mat &amp;gray) &#123; goodFeaturesToTrack(gray, features, 5000, 0.01, 10, Mat(), 3, false);&#125;void drawFeatures(Mat &amp;frame) &#123; for (int i = 0; i &lt; fpts[0].size(); i++) &#123; circle(frame, fpts[0][i], 2, Scalar(0, 0, 255), 2); &#125;&#125;void KLTtracker() &#123; calcOpticalFlowPyrLK(pregray, gray, fpts[0], fpts[1], status, errors); int k = 0; for (int i = 0; i &lt; fpts[1].size(); i++) &#123; double dist = abs(fpts[0][i].x - fpts[1][i].x) + abs(fpts[0][i].y - fpts[1][i].y); if (dist &gt; 2 &amp;&amp; status[i]) &#123; iniPoints[k] = iniPoints[i]; fpts[1][k++] = fpts[1][i]; &#125; &#125; //保存特征点并绘制跟踪轨迹 iniPoints.resize(k); fpts[1].resize(k); drawTrackLine(); //fpts[0] = fpts[1]; swap(fpts[1], fpts[0]);&#125;void drawTrackLine() &#123; for (int i = 0; i &lt; fpts[1].size(); i++) &#123; line(frame, iniPoints[i], fpts[1][i], Scalar(0, 255, 0), 2); circle(frame, features[i], 2, Scalar(0, 0, 255), 2); &#125;&#125; 效果图]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV学习之路(四)——视频操作基础]]></title>
    <url>%2F2019%2F04%2F11%2FOpenCV%2FOpenCV%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%E8%A7%86%E9%A2%91%E6%93%8D%E4%BD%9C%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[OpenCV视频操作基础VideoCapture作用是从视频文件或从摄像头捕获视频并显示出来 读取并播放视频先实例化在初始化1234567VideoCapture capture;capture.open(&quot;MV.mp4&quot;);if (!capture.isOpened()) &#123; printf(&quot;Open ERROR!\n&quot;); return -1; &#125; 在视频读入到VideoCapture类对象之后，紧接着可以用一个循环将每一帧显示出来。同时可以对每一帧做图像处理，就和普通一张图片一样。123456789101112131415Mat frame,gray,bin;double FPS = capture.get(CV_CAP_PROP_FPS);printf(&quot;FPS:%f&quot;,FPS);namedWindow(&quot;Video&quot;, CV_WINDOW_AUTOSIZE);while (capture.read(frame))&#123; cvtColor(frame, gray, COLOR_BGR2GRAY); threshold(gray, bin, 0, 255, THRESH_BINARY | THRESH_OTSU); imshow(&quot;Video&quot;, bin);char c = waitKey(100);//100ms 返回值为当前键盘按键值 ESC ASCII码是27if (c == 27) &#123; break; &#125;&#125; 调用摄像头采集图像区别仅仅是在VideoCapture类初始化对象时填入一个0。12VideoCapture capture;capture.open(0);]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV学习之路(五)——BSM背景消去建模]]></title>
    <url>%2F2019%2F04%2F11%2FOpenCV%2FOpenCV%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94BSM%E8%83%8C%E6%99%AF%E6%B6%88%E5%8E%BB%E5%BB%BA%E6%A8%A1%2F</url>
    <content type="text"><![CDATA[背景消除基本原理Opencv–背景消除建模（BSM）BS ,Background Subtraction 背景消除在opencv中有两种方法可以进行背景消除：其一、基于机器学习(KNN–K个最近邻)背景消除建模其二、基于图像分割(GMM，高斯混合模型抗干扰图像分割)背景消除建模 相关API 1、BackgroundSubtractor 2、BackgroundSubtractorMOG2 图像分割方法 3、BackgroundSubtractorKNN 机器学习（KNN）最近邻方法其中GMM法可以有效抗噪，防止每帧之间微弱变化带来的噪声影响。 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041#include "opencv2/highgui/highgui.hpp"#include "opencv2/opencv.hpp"#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv)&#123; VideoCapture capture; capture.open("video.avi"); if (!capture.isOpened()) &#123; printf("Open ERROR!\n"); return -1; &#125; Mat frame; Mat bsmaskMOG2; double FPS = capture.get(CV_CAP_PROP_FPS); printf("FPS:%f",FPS); namedWindow("Origin Video", CV_WINDOW_AUTOSIZE); namedWindow("BSM Video", CV_WINDOW_AUTOSIZE); Ptr&lt;BackgroundSubtractor&gt;pMOG2 = createBackgroundSubtractorMOG2(); while (capture.read(frame)) &#123; //cvtColor(frame, gray, COLOR_BGR2GRAY); //threshold(gray, bin, 0, 255, THRESH_BINARY | THRESH_OTSU); imshow("Origin Video", frame); pMOG2-&gt;apply(frame, bsmaskMOG2); imshow("BSM Video", bsmaskMOG2); char c = waitKey(100);//100ms 返回值为当前键盘按键值 ESC ASCII码是27 if (c == 27) &#123; break; &#125; &#125; waitKey(0); return 0;&#125; 对视频进行形态学滤波处理 开运算，即为先腐蚀后膨胀的过程，可以用来消除小物体（类似椒盐噪声，噪点），在纤细点处分离物体，并且在平滑较大物体的边界时不明显改变面积。 闭运算，先膨胀后腐蚀的过程称为闭运算，闭运算能够排除小型黑洞（黑色区域）。morphologyEx(bsmaskMOG2, bsmaskMOG2, MORPH_OPEN, element);输入图像，输出图像，形态学变换方法，运算内核运算内核的创建Mat element = getStructuringElement(MORPH_RECT, Size(3, 3));内核的形状，尺寸形态学滤波后代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &quot;opencv2/highgui/highgui.hpp&quot;#include &quot;opencv2/opencv.hpp&quot;#include &lt;iostream&gt;using namespace cv;using namespace std;int main(int argc, char** argv)&#123; VideoCapture capture; capture.open(&quot;video.avi&quot;); if (!capture.isOpened()) &#123; printf(&quot;Open ERROR!\n&quot;); return -1; &#125; Mat frame; Mat bsmaskMOG2; double FPS = capture.get(CV_CAP_PROP_FPS); printf(&quot;FPS:%f&quot;,FPS); namedWindow(&quot;Origin Video&quot;, CV_WINDOW_AUTOSIZE); namedWindow(&quot;BSM Video&quot;, CV_WINDOW_AUTOSIZE); Ptr&lt;BackgroundSubtractor&gt;pMOG2 = createBackgroundSubtractorMOG2(); //定义核 Mat element = getStructuringElement(MORPH_RECT, Size(5, 5)); while (capture.read(frame)) &#123; //cvtColor(frame, gray, COLOR_BGR2GRAY); //threshold(gray, bin, 0, 255, THRESH_BINARY | THRESH_OTSU); imshow(&quot;Origin Video&quot;, frame); pMOG2-&gt;apply(frame, bsmaskMOG2); //进行形态学操作 morphologyEx(bsmaskMOG2, bsmaskMOG2, MORPH_OPEN, element); imshow(&quot;BSM Video&quot;, bsmaskMOG2); char c = waitKey(100);//100ms 返回值为当前键盘按键值 ESC ASCII码是27 if (c == 27) &#123; break; &#125; &#125; waitKey(0); return 0;&#125;]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV学习之路(六)——对象检测与跟踪（基于颜色）]]></title>
    <url>%2F2019%2F04%2F11%2FOpenCV%2FOpenCV%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B%E4%B8%8E%E8%B7%9F%E8%B8%AA(%E5%9F%BA%E4%BA%8E%E9%A2%9C%E8%89%B2)%2F</url>
    <content type="text"><![CDATA[一些基本知识contours被定义成二维浮点型向量，这里面将来会存储找到的边界的（x,y）坐标。vector&lt;Vec4i&gt;hierarchy是定义的层级。这个在找边界findcontours的时候会自动生成，这里只是给它开辟一个空间。将来findContours( src, contours, hierarchy, RETR_TREE, CHAIN_APPROX_SIMPLE, Point(0, 0));就能算出边界的坐标，存在contours里面。typedef Vec&lt;int, 4&gt; Vec4i;Vec4i指的是四个整形数膨胀：高亮区域增长（取局部MAX）腐蚀：暗部地区增长（取局部MIN） 基于颜色跟踪实现步骤 inRange过滤 形态学操作提取 轮廓查找 外界矩形获取 位置标定 inRange()彩色图像分割void inRange(InputArray src, InputArray lowerb, InputArray upperb, OutputArray dst) src：输入图像，CV2常用Mat类型； lowerb：lower boundary下限，scalar类型的像素值，单通道scalar取一个值就行，彩图3通道scalar三个值； upperb：上限，类型与lowerb同理； dst：输出图像，尺寸与src一致，类型是CV_8U，但没有指定通道数。 对于多通道的输入，输出结果是各个通道的结果相与，当各通道结果都在上下限之内时，输出为255，否则为0。因此也有人将输出理解为掩码模板！ 形态学开操作和膨胀12morphologyEx(mask, mask, MORPH_OPEN, kernel1, Point(-1, -1), 1); // 开操作dilate(mask, mask, kernel2, Point(-1, -1), 4);// 膨胀 ###使用经过颜色分割后的图像来实现轮廓发现与位置标定，返回ROI矩形 1234567891011121314151617181920void processFrame(Mat &amp;binary, Rect &amp;rect) &#123; vector&lt;vector&lt;Point&gt;&gt; contours;//储存轮廓点(二维向量） vector&lt;Vec4i&gt; hireachy; //轮廓层次信息 findContours(binary, contours, hireachy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, Point(0, 0));//查找轮廓 if (contours.size() &gt; 0) &#123; double maxArea = 0.0; for (size_t t = 0; t &lt; contours.size(); t++) &#123; //几个轮廓遍历 double area = contourArea(contours[t]); if (area &gt; maxArea) &#123;//寻找面积最大的轮廓 maxArea = area; rect = boundingRect(contours[t]); &#125; &#125; &#125; else &#123; rect.x = rect.y = rect.width = rect.height = 0; &#125;&#125; 完整代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#include &lt;opencv2/opencv.hpp&gt;#include &lt;iostream&gt;using namespace std;using namespace cv;Rect roi;void processFrame(Mat &amp;binary, Rect &amp;rect);int main(int argc, char* argv) &#123; // load video VideoCapture capture; capture.open(&quot;006.mp4&quot;); if (!capture.isOpened()) &#123; printf(&quot;could not find video file&quot;); return -1; &#125; Mat frame, mask; Mat kernel1 = getStructuringElement(MORPH_RECT, Size(3, 3), Point(-1, -1)); Mat kernel2 = getStructuringElement(MORPH_RECT, Size(5, 5), Point(-1, -1)); namedWindow(&quot;input video&quot;, CV_WINDOW_AUTOSIZE); namedWindow(&quot;track mask&quot;, CV_WINDOW_AUTOSIZE); while (capture.read(frame)) &#123; inRange(frame, Scalar(0, 127, 0), Scalar(120, 255, 120), mask); // 过滤 morphologyEx(mask, mask, MORPH_OPEN, kernel1, Point(-1, -1), 1); // 开操作 dilate(mask, mask, kernel2, Point(-1, -1), 4);// 膨胀 imshow(&quot;track mask&quot;, mask); // 使用经过颜色过滤后的图像来实现轮廓发现与位置标定，返回ROI矩形 processFrame(mask, roi); //利用对角线两点来画矩形： //void rectangle(Mat&amp; img, Point pt1, Point pt2, const Scalar&amp; color, int thickness = 1, int lineType = 8, int shift = 0) // 传入矩形参数来画矩形： //void rectangle(Mat&amp; img, Rect rec, const Scalar&amp; color, int thickness = 1, int lineType = 8, int shift = 0) //得到位置后在原图像上画出ROI rectangle(frame, roi, Scalar(0, 0, 255), 3, 8, 0); imshow(&quot;input video&quot;,frame); // trigger exit char c = waitKey(100); if (c == 27) &#123; break; &#125; &#125; capture.release(); waitKey(0); return 0;&#125;void processFrame(Mat &amp;binary, Rect &amp;rect) &#123; vector&lt;vector&lt;Point&gt;&gt; contours;//储存轮廓点(二维向量） vector&lt;Vec4i&gt; hireachy; //轮廓层次信息 findContours(binary, contours, hireachy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, Point(0, 0));//查找轮廓 if (contours.size() &gt; 0) &#123; double maxArea = 0.0; for (size_t t = 0; t &lt; contours.size(); t++) &#123; //几个轮廓遍历 //double area = contourArea(contours[static_cast&lt;int&gt;(t)]); double area = contourArea(contours[t]); if (area &gt; maxArea) &#123;//寻找面积最大的轮廓 maxArea = area; //rect = boundingRect(contours[static_cast&lt;int&gt;(t)]); rect = boundingRect(contours[t]); &#125; &#125; &#125; else &#123; rect.x = rect.y = rect.width = rect.height = 0; &#125;&#125;]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV学习之路(一)——图像处理基本知识总结]]></title>
    <url>%2F2019%2F04%2F11%2FOpenCV%2FOpenCV%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[图像处理中的概念图像高频部分代表了图像的细节、纹理信息；低频代表了图像的轮廓信息。 低通-》模糊 高通-》锐化 腐蚀和膨胀是针对白色部分（高亮部分）而言的。膨胀就是对图像高亮部分进行“领域扩张”，效果图拥有比原图更大的高亮区域；腐蚀是原图中的高亮区域被蚕食，效果图拥有比原图更小的高亮区域。 开运算：先腐蚀再膨胀，用来消除小物体 闭运算：先膨胀再腐蚀，用于排除小型黑洞 形态学梯度：就是膨胀图与俯视图之差，用于保留物体的边缘轮廓。 顶帽：原图像与开运算图之差，用于分离比邻近点亮一些的斑块。 黑帽：闭运算与原图像之差，用于分离比邻近点暗一些的斑块。 rows:行cols:列 常用的数据结构1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include&lt;opencv2\opencv.hpp&gt; #include&lt;opencv2\highgui\highgui.hpp&gt;using namespace std;using namespace cv;//常见数据结构使用方法总结int main()&#123; //Mat的用法 Mat m1(2, 2, CV_8UC3, Scalar(0, 0, 255)); //其中的宏的解释：CV_[位数][带符号与否][类型前缀]C[通道数] cout &lt;&lt; m1 &lt;&lt; endl; //或者,利用IplImage指针来初始化,将IplImage*转化为Mat IplImage* image = cvLoadImage(&quot;lena.jpg&quot;); Mat mat = cvarrToMat(image); //Mat转IplImage: IplImage img = IplImage(mat); //或者 Mat m2; m2.create(4, 5, CV_8UC(2)); //点的表示:Point Point p; p.x = 1; //x坐标 p.y = 1; //y坐标 //或者 Point p2(1, 1); //颜色的表示：Scalar(b,g,r);注意不是rgb，注意对应关系 Scalar(1, 1, 1); //尺寸的表示:Size Size(5, 5);// 宽度和高度都是5 //矩形的表示：Rect，成员变量有x,y,width,height Rect r1(0, 0, 100, 60); Rect r2(10, 10, 100, 60); Rect r3 = r1 | r2; //两个矩形求交集 Rect r4 = r1 &amp; r2; //两个矩形求并集 waitKey(0); 访问图片中像素的方式Mat类有若干成员函数可以获取图像的属性。cols表示列，rows表示行，channels()返回图像的通道数，灰度图通道数1，彩色图通道数3。每行的像素值由一下语句得到：1int colNumber = outputImage.cols*outputImage*channels();//列数*通道数=每一行元素个数 （有点三维立体的感觉） 为了简化指针运算，Mat类提供了ptr函数可以得到图像任意行的首地址。ptr是一个模版函数，它返回第i行的首地址：1uchar* data = img.ptr&lt;uchar&gt;(i); //获取第i行地址 12345678910111213141516171819202122#include&lt;opencv2\opencv.hpp&gt; #include&lt;opencv2\highgui\highgui.hpp&gt;using namespace std;using namespace cv;//使用指针的方式int main()&#123; Mat img = imread(&quot;lol1.jpg&quot;); for (int i = 0; i &lt; img.rows; i++) &#123; uchar* data = img.ptr&lt;uchar&gt;(i); //获取第i行地址 for (int j = 0; j &lt; img.cols; j++) &#123; printf(&quot;%d\n&quot;,data[j]); &#125; &#125; waitKey(0);&#125; 感兴趣区域：ROI在图像处理领域，常常需要设定感兴趣区域（ROI,region of interest）来专注或者简化工作过程，即在图像中选择一个图像区域，这个区域是图像分析关注的重点。而且使用ROI指定想读入的目标，可以减少处理时间，增加精度，带来便利。定义ROI区域两种方法：1.使用表示矩形区域的Rect2.Range指定感兴趣的行或列的范围 基础图像操作创建窗口：namedWindow() void namedWindow(const String&amp; winname, int flags = WINDOW_AUTOSIZE); 因为有时候需要用到窗口的名字，尽管这个时候还没有载入图片，比如我们要在一个窗口上加入一个工具条，我们必须首先知道窗口的名字，这样才知道在哪里加上这个toolbar。 namedWindow还有一个很重要的功能，如果使用默认参数，窗口是无法自由调整的，如果想实现用户自由拉伸窗口，可以这么做：12namedWindow(&quot;srcImage&quot;, WINDOW_NORMAL);// 注意这个宏，使用WINDOW_NORMAL可以允许用户自由伸缩窗口大小 imshow(&quot;srcImage&quot;, srcImage); 输出图像到文件：imwrite()1bool imwrite( const String&amp; filename, InputArray img,const std::vector&amp;params = std::vector()); 创建trackbar以及使用下面的例子利用trakbar打开多个图片。12345678910111213141516171819202122232425262728293031#include&lt;opencv2\opencv.hpp&gt; #include&lt;opencv2\highgui\highgui.hpp&gt;using namespace std;using namespace cv;#define PIC_MAX_NUM 5int pic_num = 0;void on_track(int,void*)&#123; char file[10]; sprintf(file, &quot;%d.jpg&quot;, pic_num); Mat img = imread(file); if (!img.data) &#123; cout &lt;&lt; &quot;读取图片失败&quot; &lt;&lt; endl; return; &#125; imshow(&quot;展示多幅图片&quot;, img);&#125;int main()&#123; namedWindow(&quot;展示多幅图片&quot;); createTrackbar(&quot;图片编号&quot;, &quot;展示多幅图片&quot;, &amp;pic_num, PIC_MAX_NUM, on_track); on_track(pic_num, NULL); waitKey(0);&#125; 转为灰度图1234567891011121314151617#include&lt;opencv2\opencv.hpp&gt; #include&lt;opencv2\highgui\highgui.hpp&gt;using namespace std;using namespace cv;int main()&#123; Mat img = imread(&quot;lol1.jpg&quot;); Mat dstImg; cvtColor(img, dstImg,COLOR_BGR2GRAY);//从宏名字就可以知道，是彩色图转换到灰度图 imshow(&quot;灰度图&quot;, dstImg); waitKey(0);&#125; 图像二值化操作两种方法，全局固定阈值二值化和局部自适应阈值二值化 1.全局固定阈值很容易理解，就是对整幅图像都是用一个统一的阈值来进行二值化； 2.局部自适应阈值则是根据像素的邻域块的像素值分布来确定该像素位置上的二值化阈值。1234567891011121314151617181920212223242526272829303132#include&lt;opencv2\opencv.hpp&gt; #include&lt;opencv2\highgui\highgui.hpp&gt;using namespace std;using namespace cv;int main(int argc, char** argv)&#123; Mat image = imread(&quot;lol1.jpg&quot;, CV_LOAD_IMAGE_GRAYSCALE); //注意了，必须是载入灰度图 if (image.empty()) &#123; cout &lt;&lt; &quot;read image failure&quot; &lt;&lt; endl; return -1; &#125; // 全局二值化 int th = 100; Mat global; threshold(image, global, th, 255, CV_THRESH_BINARY_INV); // 局部二值化 int blockSize = 25; int constValue = 10; Mat local; adaptiveThreshold(image, local, 255, CV_ADAPTIVE_THRESH_MEAN_C, CV_THRESH_BINARY_INV, blockSize, constValue); imshow(&quot;全局二值化&quot;, global); imshow(&quot;局部二值化&quot;, local); waitKey(0); return 0;&#125; Canny边缘检测思路：将原始图像转化为灰度图，用blur函数进行图像模糊以降噪，然后用Canny函数进行边缘检测。需要注意的是，这个函数阈值1和阈值2两者中较小的值用于边缘连接，而较大的值用来控制强边缘的初始端，推荐的高低阈值比在2：1和3：1之间。12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;opencv2\opencv.hpp&gt;using namespace cv;using namespace std;int main()&#123; Mat SrcPic = imread(&quot;1.jpg&quot;); imshow(&quot;Src Pic&quot;, SrcPic); Mat DstPic, edge, grayImage; //创建与src同类型和同大小的矩阵 DstPic.create(SrcPic.size(), SrcPic.type()); //将原始图转化为灰度图 cvtColor(SrcPic, grayImage, COLOR_BGR2GRAY); //先使用3*3内核来降噪 edge为均值滤波后图像，需与原图像大小一样 blur(grayImage, edge, Size(3, 3)); //运行canny算子，最后一个3 是默认算子孔径 Canny(edge, edge, 3, 9, 3); imshow(&quot;边缘提取效果&quot;, edge); waitKey(); return 0;&#125; 直方图均衡化直方图 可以用来反映灰度情况（0-255分布情况），每个等级的频数分布直方图均值化拉伸图像的灰度值范围，使得两头变多（0和255），所以提高了对比度，显然均衡化后的图片对比度变高了，变得更加明亮。123456789101112131415161718192021#include&lt;opencv2\opencv.hpp&gt; #include&lt;opencv2\highgui\highgui.hpp&gt;using namespace std;using namespace cv;//直方图均衡化int main()&#123; Mat img = imread(&quot;3.jpg&quot;); imshow(&quot;原始图&quot;, img); Mat dst; cvtColor(img, img, CV_RGB2GRAY); imshow(&quot;灰度图&quot;, img); equalizeHist(img, dst); imshow(&quot;直方图均衡化&quot;, dst); waitKey(0);&#125;]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV学习之路(七)——角点检测]]></title>
    <url>%2F2019%2F04%2F11%2FOpenCV%2FOpenCV%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[角点检测（Corner Detection）是计算机视觉中用来获得图像特征的一种方法，广泛应用于运动检测、视频追踪、目标识别等领域中，也称特征点检测。 一、兴趣点与角点对于角点，到目前为止还没有明确的数学定义。但是你可以认为角点就是极值点，即在某方面属性特别突出的点。一般的角点检测都是对有具体定义的、或者是能够具体检测出来的兴趣点的检测。这意味着兴趣点可以是角点，是在某些属性上强度最大或者最小的孤立点、线段的终点，或者是曲线上局部曲率最大的点。通俗的来说，在一副图像中，我们可以认为角点是物体轮廓线的连接点（见图1），当拍摄视角变化的时候，这些特征点仍能很好地保持稳定的属性。图1角点通常被定义为两条边的交点，角点的局部领域应该具有两个不同区域的不同方向的边界。实际应用中，大多数角点检测方法检测的是拥有特定特征的图像点，不仅仅是“角点”。这些特征点在图像中具有具体的坐标，并具有某些数学特征，如局部最大或最小灰度、某些梯度特征。角点在保留图像图形重要特征的同时，可以有效地减少信息的数据量，使其信息的含量很高，有效地提高了计算的速度，有利于图像的可靠匹配，使得实时处理成为可能。 图像特征类型可以被分为如下三种： 边缘 角点（感兴趣关键点） 斑点（Blobs）（感兴趣区域） 其中，角点是个很特殊的存在。如果某一点在任意方向的一个微小变动都会引起灰度很大变化，那么就称为角点。角点位于两条边缘的交点处，代表了两个边缘变化的方向上的点，所以它们是可以精确定位的二维特征，甚至可以达到亚精度的精度。关于角点的具体描述可以有如下几种： 一阶导师（即灰度的梯度）的局部最大所对应的像素点。 两条即两条以上边缘交点。 图像中梯度值和梯度方向的变化速率都很高的点。 角点处的一阶导数最大，二阶导数为零，它指示了物体边缘变化不连续的方向。 二、角点检测在当前的图像处理领域，角点检测算法可以归纳为以下三类 基于灰度图像的角点检测 基于二值图像的角点检测 基于轮廓曲线的角点检测基于灰度图像的角点检测又可分为基于梯度、基于模版和基于模版梯度组合三类方法。基于模版的方法主要考虑像素领域点的灰度变化，即图像亮度的变化，将于邻点亮度对比足够大的点定义为角点。常见的基于模版的角点检测算法有Harris角点检测算法，KLT角点检测算法等。 三、Harris角点检测Harris角点检测是一种直接基于灰度图像的角点提取算法，稳定性高，尤其对L型角点检测精度很高。但由于采用了高斯滤波，运算速度相对较慢，角点信息有丢失和位置偏移的现象。图2 角点检测最原始的想法就是取某个像素的一个邻域窗口，当这个窗口在各个方向上进行小范围移动时，观察窗口内平均的像素灰度值的变化（即，E(u,v)，Window-averaged change of intensity）。从上图可知，我们可以将一幅图像大致分为三个区域（‘flat’，‘edge’，‘corner’），这三个区域变化是不一样的。A. 窗口图像平坦 —————E的变化不大 B.窗口图像是一条边 ————1.沿边滑动E的变化不大 2.垂直于边滑动，E的变化会很大 C.窗口图像为一个角点 ————窗口图像沿任何方向移动，E的值变化都会很大具体原理可参考下面两篇https://blog.csdn.net/linqianbi/article/details/78930239https://blog.csdn.net/pbymw8iwm/article/details/82624898 四、OpenCV实现Harris角点检测OpenCV中cornerHarris函数可用于检测图像的Harris角点。 cornerHarris函数名参数的说明：1234567void cornerHarris( InputArray src, //输入8bit 单通道灰度Mat矩阵 OutputArray dst, //用于保存Harris角点检测结果,32位单通道,大小与src相同 int blockSize, //滑块窗口的尺寸 int ksize, //Sobel边缘检测滤波器大小 double k, //Harries中间参数,经验值0.04~0.06 int borderType=BORDER_DEFAULT //插值类型 ); 完整代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;Mat image;Mat imageGray;int thresh = 200;int MaxThresh = 255;void Trackbar(int, void*); //阈值控制int main()&#123; image = imread(&quot;1.jpg&quot;); cvtColor(image, imageGray, CV_RGB2GRAY); GaussianBlur(imageGray, imageGray, Size(5, 5), 1); // 滤波 namedWindow(&quot;Corner&quot;); createTrackbar(&quot;threshold：&quot;, &quot;Corner&quot;, &amp;thresh, MaxThresh, Trackbar); imshow(&quot;Corner&quot;, image); Trackbar(0, 0); waitKey(); return 0;&#125;void Trackbar(int, void*)&#123; Mat dst, dst8u, dstshow, imageSource; dst = Mat::zeros(image.size(), CV_32FC1); imageSource = image.clone(); cornerHarris(imageGray, dst, 3, 3, 0.04, BORDER_DEFAULT); normalize(dst, dst8u, 0, 255, CV_MINMAX); //归一化 convertScaleAbs(dst8u, dstshow); imshow(&quot;dst&quot;, dstshow); //dst显示 for (int i = 0;i&lt;image.rows;i++) &#123; for (int j = 0;j&lt;image.cols;j++) &#123; if (dstshow.at&lt;uchar&gt;(i, j)&gt;thresh) //阈值判断 &#123; circle(imageSource, Point(j, i), 2, Scalar(0, 0, 255), 2); //标注角点 &#125; &#125; &#125; imshow(&quot;Corner&quot;, imageSource);&#125; 注：1.convertScaleAbs函数是OpenCV中的函数，使用线性变换转换输入数组元素成8位无符号整型。 2.cvCircle(CvArr* img, CvPoint center, int radius, CvScalar color, int thickness=1, int lineType=8, int shift=0) img为源图像指针 center为画圆的圆心坐标 radius为圆的半径 color为设定圆的颜色，规则根据B（蓝）G（绿）R（红） thickness 如果是正数，表示组成圆的线条的粗细程度。否则，表示圆是否被填充 line_type 线条的类型。默认是8 shift 圆心坐标点和半径值的小数点位数效果图： 图3 五、OpenCV实现Harris角点检测OpenCV 提供了函数： cv2.goodFeaturesToTrack()。这个函数可以帮我们使用 Shi-Tomasi 方法获取图像中 N 个最好的角点（也可以通过改变参数来使用 Harris 角点检测算法）。通常情况下，输入的应该是灰度图像。然后确定你想要检测到的角点数目。再设置角点的质量水平， 0到 1 之间。它代表了角点的最低质量，低于这个数的所有角点都会被忽略。最后在设置两个角点之间的最短欧式距离。根据这些信息，函数就能在图像上找到角点。所有低于质量水平的角点都会被忽略,然后再把合格角点按角点质量进行降序排列。函数会采用角点质量最高的那个角点（排序后的第一个），然后将它附近（最小距离之内）的角点删掉。按着这样的方式最后返回 N 个最佳角点。123456789101112131415161718//goodFeaturesToTrack有比cornerHarries更多的控制参数,函数原型: void goodFeaturesToTrack( InputArray image, OutputArray corners, int maxCorners, double qualityLevel, double minDistance, InputArray mask=noArray(), int blockSize=3, bool useHarrisDetector=false, double k=0.04); /*第一个参数image：8位或32位单通道灰度图像; 第二个参数corners: 位置点向量,保存的是检测到角点的坐标; 第三个参数maxCorners: 定义可以检测到的角点的数量的最大值; 第四个参数qualityLevel: 检测到的角点的质量等级,角点特征值小于qualityLevel*最大特征 值的点将被舍弃; 第五个参数minDistance: 两个角点间最小间距,以像素为单位; 第六个参数mask: 指定检测区域,若检测整幅图像,mask置为空Mat(); 第七个参数blockSize: 计算协方差矩阵时窗口大小; 第八个参数useHarrisDector: 是否使用Harris角点检测,为false,则使用Shi-Tomasi算子; 第九个参数K: 留给Harris角点检测算子用的中间参数,一般取经验值0.04~0.06.第8个参数为false时,改参数不起作用; 123456789101112131415161718192021222324252627282930313233343536373839#include &quot;core/core.hpp&quot; #include &quot;highgui/highgui.hpp&quot; #include &quot;imgproc/imgproc.hpp&quot; using namespace cv; Mat image;Mat imageGray;int thresh=5; //角点个数控制int MaxThresh=255; void Trackbar(int,void*); int main(int argc,char*argv[]) &#123; image=imread(argv[1]); cvtColor(image,imageGray,CV_RGB2GRAY); GaussianBlur(imageGray,imageGray,Size(5,5),1); // 滤波 namedWindow(&quot;Corner Detected&quot;); createTrackbar(&quot;threshold：&quot;,&quot;Corner Detected&quot;,&amp;thresh,MaxThresh,Trackbar); imshow(&quot;Corner Detected&quot;,image); Trackbar(0,0); waitKey(); return 0;&#125; void Trackbar(int,void*)&#123; Mat dst,imageSource; dst=Mat::zeros(image.size(),CV_32FC1); imageSource=image.clone(); vector&lt;Point2f&gt; corners; goodFeaturesToTrack(imageGray,corners,thresh,0.01,10,Mat()); for(int i=0;i&lt;corners.size();i++) &#123; circle(imageSource,corners[i],2,Scalar(0,0,255),2); &#125; imshow(&quot;Corner Detected&quot;,imageSource); &#125; goodFeaturesToTrack相比cornerHarris，增加了检测的复杂度，同时也可以更好的控制检测到的角点的特性，比如角点个数，角点间最小间距等。设置检测点数为11时，只有特征值最大的前11个角点被检测出来,继续增大检测点数的值，所有角点都被检测出来]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV学习之路(九)——CAMShift视频对象跟踪]]></title>
    <url>%2F2019%2F04%2F11%2FOpenCV%2FOpenCV%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%B9%9D%EF%BC%89%E2%80%94%E2%80%94CAMShift%E8%A7%86%E9%A2%91%E5%AF%B9%E8%B1%A1%E8%B7%9F%E8%B8%AA%2F</url>
    <content type="text"><![CDATA[一、RGB与HSV颜色系统数字图像处理中常用的采用模型是RGB（红，绿，蓝）模型和HSV（色调，饱和度，亮度），RGB广泛应用于彩色监视器和彩色视频摄像头，我们平时的图片一般都是RGB模型。而HSV模型更符合人描述和解释颜色的方式，HSV的彩色描述对人来说是自然且非常直观的，CAMshift是基于HSV颜色系统的。 HSV模型中颜色的参数分别是：色调（H：hue），饱和度（S：saturation），亮度（V：value）。由A. R. Smith在1978年创建的一种颜色空间, 也称六角锥体模型(Hexcone Model)。 图1 色调（H：hue）：用角度度量，取值范围为0°～360°，从红色开始按逆时针方向计算，红色为0°，绿色为120°,蓝色为240°。它们的补色是：黄色为60°，青色为180°,品红为300°； 饱和度（S：saturation）：取值范围为0.0～1.0，值越大，颜色越饱和。 亮度（V：value）：取值范围为0(黑色)～255(白色） OpenCV下有个函数可以直接将RGB模型转换为HSV模型，OpenCV中H∈ [0, 180）， S ∈ [0, 255]， V ∈ [0, 255]。我们知道H分量基本能表示一个物体的颜色，但是S和V的取值也要在一定范围内，因为S代表的是H所表示的那个颜色和白色的混合程度，也就说S越小，颜色越发白，也就是越浅；V代表的是H所表示的那个颜色和黑色的混合程度，也就说V越小，颜色越发黑。经过实验，一些基本的颜色H的取值可以如下设置：Green 38-75，Blue 75-130，Red 160-179 OpenCV 的文档中是这样解释的：原本输出的 HSV 的取值范围分别是 0-360， 0-1， 0-1；但是为了匹配目标数据类型 OpenCV 将每个通道的取值范围都做了修改，于是就变成了 0-180， 0-255， 0-255，并且同时解释道：为了适应 8bit 0-255 的取值范围，将 hue 通道 0-360 的取值范围做了减半处理，这就是为什么 OpenCV 中 H通道的取值范围是0-180。 图2 HSV色域范围 二、图像的前置操作 读取视频 12345678910 //打开视频VideoCapture capture;capture.open(&quot;test.mp4&quot;);if (!capture.isOpened())&#123; printf(&quot;could not find \n&quot;);&#125;//命名窗口namedWindow(&quot;Origin&quot;, CV_WINDOW_AUTOSIZE);namedWindow(&quot;ROI&quot;, CV_WINDOW_AUTOSIZE); 选中ROI区域此处直接调用selectROI,selectROI是跟踪API的一部分,需要引用Opencv_contrib库。 1234567Rect2d first = selectROI(&quot;Origin&quot;, frame);// int 转换 doubleselection.x = first.x;selection.y = first.y;selection.width = first.width;selection.height = first.height;printf(&quot;ROLx = %d;ROLy=%d,width =%d;height = %d&quot;, selection.x, selection.y, selection.width, selection.height); 将图像从RGB表示转化为HSV表示OpenCV中使用cvtColor()函数就可以实现颜色空间的转换。注意opencv中颜色方块的排列顺序是BGR，而不是熟悉的RGB，因此颜色映射码是COLOR_BGR2HSV。 12//转换到 HSVcvtColor(frame, hsv, COLOR_BGR2HSV); 提取Hue分量获取hsv图像后，需要提取出其中的Hue分量，即颜色分割。 1inRange(hsv, Scalar(0, smin, vmin), Scalar(180, smax, vmax), mask); inRange()函数的功能是检查输入数组的每个元素是不是在给定范围内。代码中可以看出，检查的是hsv的像素的Hue分量是否在0-180之间，Saturation分量是否在smin-255之间，Value分量是否在vmin和vmax之间，返回验证矩阵mask，如果hsv的像素点满足条件，那么mask矩阵中对应位置的点置255，不满足条件的置0。HSV范围是opencv中规定的，因此Hue的范围是0-180，Saturation和Value的范围是0-255。如果是Photoshop，那么HSV的范围就是0-360°和0-1了。 提取Hue分量通道复制函数mixChannels()，此函数由输入参数复制某通道到输出参数特定的通道中。1234567void mixChannels(const Mat* src, //输入的数组，所有的数组必须有相同的尺寸和深度size_t nsrcs, //第一个参数src输入的矩阵数Mat* dst, //输出的数组，所有矩阵必须被初始化，且大小和深度必须与src[0]相同size_t ndsts, //第三个参数dst输入的矩阵数const int* fromTo,//对指定的通道进行复制的数组索引size_t npairs) //第五个参数fromTo的索引数 1mixChannels(&amp;hsv, 1, &amp;hue, 1, channels, 1); 输入一个矩阵hsv,输出一个size和depth与hsv完全相同的矩阵hue，复制hsv[0]通道到hue[0]通道，也就是“提取”图像的Hue分量，储存在hue矩阵中。拿到图像Hue分量的数据后，图像的前置处理完成。 三、ROI区域直方图计算与绘制图像的前置处理完成并获取hue矩阵后，需要计算ROI区域关于hue的一维颜色直方图。这个直方图除非重新框选，否则在循环中只计算和绘制一次。calcHist()函数原型如下：123456789101112calcHist( const Mat* images, //输入的数组 int nimages, //输入数组的个数 const int* channels, //需要统计的通道索引 InputArray mask, //掩膜 OutputArray hist, //输出的目标直方图 int dims, //需要计算的直方图维度 const int* histSize, //在每一维上直方图的个数。如果是一维直方图，就是竖条(bin)的个数。 const float** ranges, //每一维数值的取值范围数组 bool uniform, //直方图是否均匀的标识符 bool accumulate //是否累加。如果为true，在下次计算的时候不会首先清空hist） 123456789101112131415161718192021222324252627//ROI直方图计算 Mat roi(hue, selection); Mat maskroi(mask, selection); //输入的数组&amp;roi只有一个，统计的通道是0通道 //使用的掩膜是maskroi，输出一维直方图，有16个竖条，取值范围是&#123;0，180&#125;。 calcHist(&amp;roi, 1, 0, maskroi, hist, 1, &amp;bins, &amp;Phranges); //直方图之后再归一化到0-255 normalize(hist, hist, 0, 255, NORM_MINMAX); //画出直方图 //每个条的宽度 int binw = drawing.cols / bins; //定义一个缓冲单bin矩阵，1行16列,用于存放颜色数据，用于直方图hsize个bin的“染色”。 Mat colorIndex = Mat(1, bins, CV_8UC3); for (int i = 0;i &lt; bins;i++) &#123; colorIndex.at&lt;Vec3b&gt;(0, i) = Vec3b(saturate_cast&lt;uchar&gt;(i * 180 / bins), 255, 255); &#125; cvtColor(colorIndex, colorIndex, COLOR_HSV2BGR); for (int i = 0;i &lt; bins; i++) &#123; //val是直方图hist的相对histimg的高度。hist.at(i)获取了第i个bin直方图数据，除以255后得到百分比 //再乘以histimg的行数就得到了相对高度，最后进行int的强制类型转换，转换为整数。 int val = saturate_cast&lt;int&gt;(hist.at&lt;float&gt;(i)*drawing.rows / 255); //之后使用rectangle()函数进行16个bin的绘制值得注意的是矩阵的坐标系以左上角为原点，y轴是向下的，而需要展示给人看的直方图图案是左下角为原点，y轴向上的 //因此rectangle的两个标定点的纵坐标是histimg.rows和(histimg.rows - val)而不是0和val。 rectangle(drawing, Point(i*binw, drawing.rows), Point((i + 1)*binw, drawing.rows - val), Scalar(colorIndex.at&lt;Vec3b&gt;(0, i)), -1, 8, 0); &#125; 四、反向投影calcBackProject()函数原型如下：12345678910void calcBackProject(const Mat* images, //输入的数组int nimages, //输入数组的个数const int* channels, //需要统计的通道索引InputArray hist, //输入的直方图OutputArray backProject,//目标的反向投影const float** ranges, //每一位数值的取值范围double scale=1, //输出方向投影的缩放因子bool uniform=true //指示直方图是否均匀的标识符) 12calcBackProject(&amp;hue, 1, 0, hist, backprojection, &amp;Phranges); backprojection &amp;= mask; 五、CAMshift目标追踪12345RotatedRect CamShift( InputArray _probImage, Rect&amp; window, TermCriteria criteria) CamShift函数接受3个参数：反向投影，矩形搜索框，和迭代中止条件。TermCriteria( CV_TERMCRIT_EPS | CV_TERMCRIT_ITER, 10, 1 ))示例意思：精度先达到1或者迭代次数先达到10次时，停止迭代获取CamShift的返回值，是一个旋转矩形，根据旋转矩形绘制一个椭圆形显示在图像上作为追踪结果。 1234//CamShift函数接受3个参数：反向投影，矩形搜索框，和迭代中止条件。RotatedRect trackBox = CamShift(backprojection, selection, TermCriteria((TermCriteria::COUNT | TermCriteria::EPS), 10, 1));//获取CamShift的返回值，是一个旋转矩形，根据旋转矩形绘制一个椭圆形显示在图像上作为追踪结果。ellipse(frame, trackBox, Scalar(0, 0, 255), 3, 8);]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
</search>
